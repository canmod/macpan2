---
title: "Calibrating Compartmental Models to Data"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Calibrating Compartmental Models to Data}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

[![status](https://img.shields.io/badge/status-working%20draft-red)](https://canmod.github.io/macpan2/articles/vignette-status#working-draft)

```{r opts, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Setup:

```{r pkgs, message = FALSE}
library(macpan2)
library(macpan2helpers)
library(dplyr)
library(ggplot2); theme_set(theme_bw())
```

In addition to these packages, the `broom.mixed` package is used below (if installed).

## 'Hello, World': an easy (??) calibration exercise

We'll do the first thing you should always do when trying out a new fitting procedure: simulate clean, nice data from the model and see if you can recover something close to the true parameters.

### Step 0: set up simulator and generate 'data'

First set up the model from the quickstart guide:

```{r sir_setup}
sir = Compartmental(system.file("starter_models", "sir", package = "macpan2"))
sir_simulator = sir$simulators$tmb(
  time_steps = 100,
  state = c(S = 99, I = 1, R = 0),
  flow = c(foi = NA, gamma = 0.1),
  beta = 0.2,
  N = empty_matrix
  )
## This is key!
sir_results = sir_simulator$report(.phases = "during")
```

**fixme**: providing mismatched time series (e.g. by forgetting to specify `.phases = "during"` when generating simulated data) gives cryptic/confusing errors and behaviour (warnings about failure to recycle, objective function values equal to zero). (We can make a reprex for this by leaving out `.phases = "during"` above ...

Add some noise to the prevalence (`I`) value:
```{r sir_noise}
set.seed(101)
sir_prevalence = (sir_results
    |> dplyr::select(-c(matrix, col))
    |> filter(row == "I")
    |> mutate(obs_val = value + rnorm(n(), sd = 1))
)
gg0 <- ggplot(sir_prevalence, aes(time)) +
    geom_point(aes(y = obs_val)) +
    geom_line(aes(y = value))
print(gg0)
```

### Step 1: add observed data and slots for history etc.

While the files specified in the model definition (`variables.csv`, `derivations.csv`, `settings.json`, `flows.csv`) are sufficient to define a simulator, we now need to add more structure to the model object so we can do the calibration - specifically, both whatever observed data we want to compare against, and whatever new variables ("matrices") and expressions we will evaluate to compute the goodness of fit (aka the loss function or objective function) of a particular set of parameters.

**fixme**: any chance of adding an interface layer that would do this stuff?

If we have a `TMBSimulator` object (i.e., `sir_simulator` in this example), the `$add$matrices()` method will add new variables to the space where the object has already stored the state variables, etc. (you use `sir_simulator$matrix_names()` to list the existing matrices, although this produces a long, scary list of internal variables that `macpan2` has constructed)

Now we will use `$add$matrices()` to:

(a) add observed data 
(b) declare a matrix storing the simulation history of variables to compare with observed data
(c) declare a matrix to store the log-likelihood
(d) specify which matrices to save and/or return in the report

```{r sim_addmat}
sir_simulator$add$matrices(
  ## observed data
  I_obs = sir_prevalence$obs_val,
  ## simulated trajectory to compare with data
  I_sim = empty_matrix, 
  ## matrix to contain the log likelihood values at each time step
  log_lik = empty_matrix,
  ## need to save the simulation history of each of these matrices
  .mats_to_save = c("I_sim", "log_lik"),
  .mats_to_return = c("I_sim", "log_lik")
)
```
**fixme**: SJW says we don't need `match` any more?

**fixme**: some examples show `sir_simulator$print$matrix_dims()` at this point.  What is this good for/how do we interpret it?

### Step 2: collect simulated values

Collect simulated values into matrices to be
compared with data. the `.at = Inf` and
`.phase = "during"` indicates that this expression
should come at the *end* of the expressions evaluated
during each iteration of the simulation loop.

Like `$add$matrices()`, `$insert$expressions` adds components to an existing `TMBSimulator` object - in this case, expressions that will be computed during the simulation. (In this example, since we set `.phase = "during"`, the expressions will be computed at each time step.)

```{r add_Isim}
sir_simulator$insert$expressions(
  I_sim ~ I,
  .phase = "during",
  .at = Inf
)
```

**fixme**: what's the best way to handle irregularly sampled data/match up with timestamps of observed data? (not for here, this should be a footnote or put in an 'extra tricks' section)

## Step 3: set up and compute objective function

We will use the log (by default) of the Gaussian density of the
observed `I` values with mean (i.e. predicted)
value of the simulated `I` values (this is equivalent to
least-squares estimation, with the added complication that
we estimate the standard deviation explicitly rather than computing
it from the residuals). 

- Add the new parameter (standard deviation of the observed `I` distribution around the predicted `I` values)
- The `rbind_time` function gathers together the
full simulation history of the `I_sim`
matrix by binding together the rows at each
iteration.

```{r compute_loglik}
sir_simulator$add$matrices( I_sd = 1 )
sir_simulator$insert$expressions(
  log_lik ~ dnorm(I_obs, rbind_time(I_sim), I_sd),
  .phase = "after"
  )
```

Define the objective function (which will almost always be the sum
of the negative log-likelihoods for each point):

```{r add_loglik}
sir_simulator$replace$obj_fn(~ -sum(log_lik))
```

## Step 4: declare and/or transform parameters to be optimized, set starting values

- We could have postponed defining `I_sd` in the model until this step (**fixme**: right?), but it would have been confusing since we used it in the objective function.
- For parameters that are restricted to be positive, it is almost always best to estimate them on the log scale. This ensures that their values will always be non-negative (and positive unless the transformed values are negative and large enough in magnitude that `exp(x)` underflows to zero) and has other advantages in optimization (**fixme**: how much detail is needed here? Shrink scale of optimization, make parameter magnitudes ${\cal O}(1)$, make Wald estimation more reliable ...)
- In practice we would often read the parameter starting values in from a CSV file (using `read.csv` from base R or `readr::read_csv()` from tidyverse), but here we can set up the data frame on the fly

```{r step4}
sir_simulator$add$transformations(Log("I_sd"))
sir_simulator$add$transformations(Log("beta"))
params <- read.delim(sep = "|", header = TRUE,
                     strip.white = TRUE, ## important!
                     text = "
mat       | row | col | default
log_I_sd  | 0   | 0   | 0
log_beta  | 0	| 0   | 1
")
sir_simulator$replace$params_frame(params)
```

Using `$add_transformations(Log("var"))` automatically adds a variable called `log_var` to the list of matrices.

**fixme**: if I fail to use `strip.white = TRUE` (so the `mat` names have trailing whitespace), then on the last line above I get a less-than-perfectly-useful error message:

> Error in valid\$consistency_params_mats\$check(self\$model) : 
> optimization parameters are not consistent with matrices
> but validity could not be checked because:<br>
> Error in if (any(!valid_pars)) { : missing value where TRUE/FALSE needed


### Step 5: do the fit

It's always a good idea to do some quick sanity checks on the objective function before you try to optimize: do you get finite values (for reasonable inputs)? Does changing the inputs change the returned value?

```{r test_objective}
sir_simulator$objective(c(1,1))
sir_simulator$objective(c(0,1))
```

```{r fit, eval = FALSE}
## these were some other things I tried out when debugging ...
print(sir_simulator$get$initial("log_I_sd"))
print(sir_simulator$get$initial("log_beta"))
print(sir_simulator$current$params_frame())
sir_simulator$report()
```

```{r sir_fit, results = "hide"}
fit <- sir_simulator$optimize$nlminb()
```


Print the results of `nlminb` - **always check the value of the convergence code** (if it's not 0, then something *may* have gone wrong ...)

```{r check_fit}
print(fit)
```

Back-transform the parameters:

```{r params}
exp(fit$par)
```

Get more information:

```{r params_fancy}
ff <- sir_simulator$ad_fun()
class(ff) <- "TMB"
if (requireNamespace("broom.mixed")) {
    (broom.mixed::tidy(ff, conf.int = TRUE)
        |> select(-c(type, std.error))
        ## ugh, shouldn't need to do this by hand
        |> mutate(term = c("I_sd", "beta"))
        |> mutate(across(where(is.numeric), exp))
    )
}
```

These correspond to true values of 0.2, 1, so pretty close.

**fixme**: can we handle back-transformation/parameter naming more nicely? `coef()`, `summary()` methods ... ??

The best-fit parameters are stored *internally*, so if we re-run the `$report()` method we will get information about the predicted best-fit trajectory:

```{r plot_results}
sim_vals <- (sir_simulator$report(.phases = "during")
    |> filter(matrix == "state", row == "I")
)
gg0 + geom_line(data = sim_vals, aes(y= value), colour = "red")
```

**fixme**: what next? Show a fit to real data? Comparison with [fitode](https://github.com/parksw3/fitode) ?

**fixme**: examples of (statistical) diagnostics? e.g. compute residuals, plot vs. time or vs fitted values, scale-location plot, etc. ?

**fixme**: add some *intermediate* examples (simple time-varying parameters, Poisson/negative binomial responses, irregularly sampled data, fitting prevalence/hospitalization/death, fitting multiple data streams ...) try to minimize 'boiler plate', focus on added features in each model ...

## Measles Data

Here is a reasonably difficult problem -- fit an SIR model to weekly measles incidence data from London UK over about six decades.

<!-- FIXME: store data locally, don't rely on IIDA ... -->
```{r sir_plot, fig.width=6}
measles = read.csv(
  file.path(
    "https://raw.githubusercontent.com/davidearn/iidda/master/data",
    "meas_uk__lon_1944-94_wk/source-data/meas_uk__lon_1944-94_wk.csv"
  ),
  comment = "#"
)
measles$date = as.Date(sprintf(
  "%s-%s-%s", measles$year, measles$month, measles$day
))
plot(measles$date, measles$cases, type = "l")
```

We need to slightly extend the standard SIR model to include waning immunity.

```{r sir_waning}
sir = Compartmental(system.file("starter_models", "sir_waning", package = "macpan2"))
sir$flows()
```


We use [radial basis functions](https://canmod.github.io/macpan2/articles/time_varying_parameters.html#radial-basis-functions-for-flexible-time-variation-in-progress) to model time-variation in the transmission rate.  We also make a variety of questionable assumptions (TODO: fix these), but the point at the moment is just to illustrate usage and provide a proof of concept.
```{r rbf}
d = 100
n = nrow(measles)
simulator = sir$simulators$tmb(
      time_steps = n
    , state = c(S = 100000 - 500, I = 500, R = 0)
    , flow = c(foi = NA_real_, gamma = 0.2, wane = 0.01)
    
    ## this beta does not matter because we will overwrite
    ## it with the output of the radial basis functions
    , beta = NA_real_
    
    ## FIXME: this is surely not the population of London at
    ##        all in the series
    , N = 100000 
    
    ## matrices involved in radial basis functions
    , X = rbf(n, d)
    , b = rnorm(d, sd = 0.01)
    , incidence = empty_matrix
    , eta = empty_matrix
    
    , .mats_to_save = c("state", "incidence", "beta")
    , .mats_to_return = c("state", "incidence", "beta")
    
## initial S is a function of initial I, which we
## fit to data below
)$insert$expressions(
    S ~ N - I
  , .phase = "before"
  , .at = 1
  
## radial basis function evaluations
)$insert$expressions(
    eta ~ gamma * exp(X %*% b)
  , .phase = "before"
  , .at = Inf
)$insert$expressions(
    beta ~ eta[time_step(1)] / clamp(S/N, 1/100)
  , .phase = "during"
  , .at = 1
  
## save the simulated incidence trajectory to
## compare with data
)$insert$expressions(
    incidence ~ I
  , .vec_by_states = "total_inflow"
  , .phase = "during"
  , .at = Inf
)
```

Here is an example simulation from this model, before fitting to data.
```{r rbf_ex, fig.width=6}
set.seed(1L)
simulated_incidence = filter(simulator$report(.phases = "during"), matrix == "incidence")$value
plot(measles$date, simulated_incidence, type = "l", xlab = "time")
```

It looks nothing like the observed measles series, but illustrates the ability to generate complex incidence patterns not present in the simple SIR model without radial basis functions and waning immunity.


```{r rbf_model, echo=FALSE}
simulator = ("https://github.com"
  |> file.path("canmod/macpan2")
  |> file.path("raw/main")
  |> file.path("misc/saved-simulators/measles-model-object.rds")
  |> url('rb')
  |> readRDS()
)
```

We modify the simulation object to be able to fit to the measles data.

```{r rbf_modify, eval=FALSE}
simulator$add$matrices(
    reports = measles$cases
  , log_lik = empty_matrix
  , sim_reports = empty_matrix
)
simulator$insert$expressions(
    sim_reports ~ rbind_time(incidence)
  , .phase = "after"
  , .at = Inf
)
simulator$replace$params(
  default = c(
      c(0.2, 0.01)
    , rep(0, d)
    , 500
  )
  , mat = c(
      rep("flow", 2L)
    , rep("b", d)  
    , "state"
  )
  , row = c(
      (1:2)
    , seq_len(d) - 1L
    , 1
  )
)
simulator$replace$obj_fn(~ - sum(dpois(reports, sim_reports)))
```

The optimization takes quite a few minutes, and still doesn't converge in 10000 function evaluations.
```{r rbf_optim_fake, eval=FALSE}
simulator$optimize$nlminb(control = list(eval.max = 10000, iter.max = 10000, trace = 10))
```



```{r rbf_opt_get}
simulator$optimization_history$get()[[3]]  ## the 3 is there because we tried two other times
```

Here the red data are fitted and black observed.
```{r plot_rbf_res, fig.width=6}
simulated_incidence = filter(simulator$report(.phases = "during"), matrix == "incidence")$value
plot(measles$date, measles$cases, xlab = "time", type = "l")
lines(measles$date, simulated_incidence, col = 2)
```

Not a perfect fit, but not bad for now (TODO: work on this, without papering over the real challenges).

## Challenging Logistic Variation in Transmission Rate

Here we consider the problem of fitting an SIR model to a simulated dataset from this model, such that the simulations pose challenges to the fitting machinery.

```{r logistic_ex}
sir = Compartmental(system.file("starter_models", "sir", package = "macpan2"))
sir$flows_expanded()
```

Our simulation model includes a logistically time-varying transmission rate.

```{r logistic_sim}
n = 2500
set.seed(1L)
simulator = sir$simulators$tmb(
      time_steps = n
    , state = c(S = 100000 - 500, I = 500, R = 0)
    , flow = c(foi = NA, gamma = 0.2)#, wane = 0.01)
    , beta = 1
    , N = 100000
    , X = cbind(1, scale(seq_len(n)))
    , b = c(0, 1)
    , incidence = empty_matrix
    , beta_values = empty_matrix
    , .mats_to_save = c("state", "incidence", "beta")
    , .mats_to_return = c("state", "incidence", "beta")
)$insert$expressions(
    beta_values ~ 1 / (1 + exp(-X %*% b))
  , .phase = "before"
  , .at = Inf
)$insert$expressions(
    beta ~ beta_values[time_step(1)]
  , .phase = "during"
  , .at = 1
)$insert$expressions(
    incidence ~ I
  , .vec_by_states = "total_inflow"
  , .phase = "during"
  , .at = Inf
)$replace$params(
    default = c(0, 1)
  , mat = rep("b", 2)
  , row = 0:1
)
```

```{r logistic_plot2, fig.height=8, fig.width=6}
set.seed(5L)
sims = simulator$report(.phases = "during")
(sims
  |> mutate(variable = if_else(matrix == "state", row, matrix))
  |> ggplot()
  + facet_wrap(~ variable, ncol = 1, scales = 'free')
  + geom_line(aes(time, value))
)
```

Fitting to the simulation data, manages to converge, but to the wrong value.
```{r logistic_bad_converge, fig.width=6}
set.seed(3L) ## different seeds do result in convergence on the correct value
reports = filter(sims, matrix == "incidence")$value
simulator$add$matrices(reports = reports, report_sim = empty_matrix)
simulator$insert$expressions(
    report_sim ~ rbind_time(incidence)
  , .phase = "after"
  , .at = Inf
)
simulator$replace$obj_fn(~ -sum(dpois(reports, report_sim)))
simulator$replace$params(
    default = rnorm(2L) ## random starting values for the optimizer
  , mat = rep("b", 2)
  , row = 0:1
)
simulator$optimize$nlminb()
simulator$current$params_frame()
fitted_incidence = (simulator$current$params_vector()
  |> simulator$report()
  |> filter(matrix == "incidence")
  |> pull(value)
)
plot(reports, type = "l")
lines(fitted_incidence, col = 2)
```

The fit is not good!  Why?  To find out we plot the likelihood surface with arrows representing the magnitude and direction of the down-hill gradient towards the optimum. Notice the very flat gradient in the direction along the valley containing the optimum at $(0, 1)$. The gradient is pointing towards the valley but not along it.  I do not understand why.

```{r logistic_plot_surf, results='hide', message=FALSE}
make_liksurf <- function(lwr = c(-1, 0), upr = c(1, 2),
                         n = c(41, 41)) {
    lik_surf = expand.grid(
        intercept_parameter = seq(from = lwr[1], to = upr[1], 
                                  length.out = n[1]),
        slope_parameter = seq(from = lwr[2], to = upr[2],
                              length.out = n[2])
    )
    gr = t(apply(lik_surf, 1, simulator$gradient))
    lik_surf$z = apply(lik_surf, 1, simulator$objective)
    gr = 0.1 * gr / max(abs(gr))
    lik_surf$gx = gr[,1]
    lik_surf$gy = gr[,2]
    return(lik_surf)
}
lik_surf <- make_liksurf()
```



```{r logistic_plot_surf2, fig.height=6, fig.width=7}
mk_plot <- function(dd, arrows = TRUE, contours = TRUE,
                    arrow_len = 0.05,
                    cbrks = (1:10)*1e5) {
    gg0 <- (ggplot(dd, aes(intercept_parameter, slope_parameter))
        + geom_tile(aes(fill = z))
        + theme_bw()
        + scale_x_continuous(expand = c(0,0))
        + scale_y_continuous(expand = c(0,0))
        + annotate(geom = "point", x = 0, y = 1, colour = "yellow", size = 2,
                   pch = 16)
        + scale_fill_continuous(trans = "log10")

    )
    if (contours) {
        gg0 <- gg0 + geom_contour(aes(z = z), colour = "red",
                                  breaks = cbrks)
    }
    if (arrows) {
        gg0 <- gg0 + geom_segment(
                         data = dd[seq(nrow(dd)) %% 5 == 0 , ],
                         aes(
                             xend = intercept_parameter - gx, 
                             yend = slope_parameter - gy
                         ), 
                         arrow = arrow(length = unit(arrow_len, "inches")), 
                         colour = 'white'
                     )
    }
    gg0
}
print(mk_plot(lik_surf))
```

```{r results = "hide"}
lik_surf2 <- make_liksurf(lwr = c(-0.1, 0.9), upr = c(0.1, 1.1))
print(mk_plot(lik_surf2, arrows = FALSE, cbrks = 1e4*(1:10)))
```

**experimental**: working with `DEoptim` (haven't included yet because I don't want to have to `Suggest:` DEoptim ...

```{r eval=FALSE}
library(DEoptim)
set.seed(101)
fit <- DEoptim(simulator$objective, lower = rep(-10, 2), upper = rep(10, 2))
fitted_incidence = (simulator$ad_fun()$env$last.par.best
    |> simulator$report()
    |> filter(matrix == "incidence")
    |> pull(value)
)
plot(reports, type = "l")
lines(fitted_incidence, col = 2)
```

* **fixme**: also add multi-start example (i.e. less fancy than DEoptim, suitable for surfaces like this one that are multimodal but smooth (ref. Raue et al. "Lessons Learned from Quantitative Dynamical Modeling in Systems Biology" 2013)?)
* **fixme**: warning that `$current$params_vector()` is mutable/unreliable. Extractor for `$ad_fun()$env$last.par.best` (or equivalently for the optim fit)?


* **fixme**: add (abbreviated) `sessionInfo` output? (Package versions?)
